# Lambda - Layer 2: Generate Meta Data for ML Training Pipeline

## Pattern

Two S3 buckets with Lambda Triggers:

* One for the initial video uploads that trigger a Python Lambda for to generate Meta Data for training
* Another for meta data generated by by an external AI system that triggers a Python Lambda for futher video file training.

## Prereqs

Configure Repo Action Secrets:

* `AWS_ACCESS_KEY_ID`: AWS CLI deployment user access key
* `AWS_SECRET_ACCESS_KEY`: AWS CLI deployment user secret key
* `LAMBDA_STAGE_DOTENV`: Stage pipeline `.env` file contents
* `LAMBDA_PROD_DOTENV`: Prod pipeline `.env` file contents

## Stand Up CI/CD Pipeline

* Create `stage` and `prod` branches as needed to deploy IaC and application pipelines
* Subsequent merges into the each branch will build and deploy the Lambda application in the typical CI/CD pattern.
* Any necessary VPC and security group settings will be automatically ingested via the networking repo state file found in the `global` S3 bucket.

## Teardown Process

Before tearding down the labmda deployment, the higher levels of the stack must be torn down first: `www` -> `api`

1. Create the following branch name to tear down the corresponding environment: i.e. `destroy-stage` or `destroy-prod`
2. All S3 upload buckets will be preserved so it can be imported later
3. DBs are not created as part of the Terraform Infrastruce so the same connection strings is used to stand data back up
4. The state file buckets are untouched as they were created with a sepereate Terraform processing on the `seed` branch so re-seeding will not be necessary to recover staging or production.